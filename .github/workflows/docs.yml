name: Deploy Documentation

on:
  push:
    branches: [main]
    paths: 
      - 'docs/**'
      - 'mkdocs.yml'
      - 'src/**/*.py'
      - '.github/workflows/docs.yml'
  pull_request:
    paths:
      - 'docs/**' 
      - 'mkdocs.yml'
      - 'src/**/*.py'
      - '.github/workflows/docs.yml'

permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  # Build documentation
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          # Fetch full history for accurate git blame analysis in docs
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true

      - name: Install documentation dependencies
        run: |
          uv pip install --system \
            mkdocs-material==9.5.* \
            mkdocstrings[python]==0.26.* \
            mkdocs-mermaid2-plugin==1.1.* \
            pymdown-extensions==10.11.*

      - name: Install project dependencies for API docs
        run: |
          uv pip install --system -e .

      - name: Setup Pages
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: actions/configure-pages@v5

      - name: Build documentation
        run: mkdocs build --strict

      - name: Upload documentation artifact
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: actions/upload-pages-artifact@v3
        with:
          path: site/

  # Deploy to GitHub Pages (only on main branch pushes)
  deploy:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

  # Link checker for pull requests and main branch (temporarily disabled)
  link-check:
    runs-on: ubuntu-latest
    needs: build
    if: false  # Temporarily disabled until artifact extraction is fixed
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install link checker
        run: |
          pip install requests beautifulsoup4 urllib3

      - name: Download built site
        uses: actions/download-artifact@v4
        with:
          name: github-pages
          path: site-archive/

      - name: Extract site archive
        run: |
          cd site-archive
          # GitHub Pages artifact is already extracted by download-artifact action
          # Just find the site directory and move it
          if [ -f artifact.tar ]; then
            tar -xf artifact.tar
          fi
          # Find the site directory and move it to parent
          find . -name "*.html" -type f | head -1 | xargs dirname | xargs -I {} mv {} ../site 2>/dev/null || \
          find . -type d -name "*" | head -2 | tail -1 | xargs -I {} mv {} ../site 2>/dev/null || \
          mv * ../site/ 2>/dev/null || \
          echo "Site files are already in place"

      - name: Check internal links
        run: |
          python - << 'EOF'
          import os
          import re
          from pathlib import Path
          from urllib.parse import urljoin, urlparse
          import requests
          from bs4 import BeautifulSoup
          
          def check_internal_links(site_dir):
              """Check all internal links in the built documentation."""
              site_path = Path(site_dir)
              broken_links = []
              
              for html_file in site_path.rglob("*.html"):
                  with open(html_file, 'r', encoding='utf-8') as f:
                      soup = BeautifulSoup(f.read(), 'html.parser')
                  
                  # Check all links
                  for link in soup.find_all('a', href=True):
                      href = link['href']
                      
                      # Skip external links and fragments
                      if href.startswith(('http', 'mailto:', '#')):
                          continue
                      
                      # Resolve relative paths
                      if href.startswith('/'):
                          target_path = site_path / href.lstrip('/')
                      else:
                          target_path = html_file.parent / href
                      
                      # Remove fragment identifier
                      if '#' in str(target_path):
                          target_path = Path(str(target_path).split('#')[0])
                      
                      # Check if target exists
                      if not target_path.exists():
                          # Try with .html extension for directory links
                          if target_path.is_dir():
                              target_path = target_path / 'index.html'
                          elif not target_path.suffix:
                              target_path = target_path.with_suffix('.html')
                      
                      if not target_path.exists():
                          broken_links.append(f"{html_file.relative_to(site_path)}: {href}")
              
              return broken_links
          
          broken = check_internal_links('site')
          if broken:
              print("Broken internal links found:")
              for link in broken:
                  print(f"  - {link}")
              exit(1)
          else:
              print("All internal links are valid!")
          EOF

  # Spell check for documentation content
  spell-check:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install aspell
        run: |
          sudo apt-get update
          sudo apt-get install -y aspell aspell-en

      - name: Create aspell dictionary
        run: |
          cat > .aspell.en.pws << 'EOF'
          personal_ws-1.1 en 50
          autosquash
          mkdocs
          textual
          TUI
          CLI
          API
          hunk
          hunks
          squash
          squashing
          rebase
          rebasing
          diff
          diffs
          pipx
          uv
          pymdownx
          mermaid
          docstrings
          GitOps
          BlameAnalyzer
          HunkParser
          RebaseManager
          DiffHunk
          HunkMappingWidget
          DiffViewer
          ProgressIndicator
          AutoSquashApp
          ApprovalScreen
          github
          workflow
          workflows
          argcomplete
          setuptools
          pyproject
          TOML
          EOF

      - name: Check spelling
        run: |
          find docs -name "*.md" -exec aspell --home-dir=. --personal=.aspell.en.pws --lang=en_US --mode=markdown --check {} \;
        continue-on-error: true